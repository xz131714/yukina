---
title: "ğŸš€ æ•°æ®æŒ–æ˜ï¼ˆDataMingï¼‰"
description: "æ•°æ®æŒ–æ˜æ˜¯ä»å¤§é‡æ•°æ®ä¸­æå–æœ‰ä»·å€¼ä¿¡æ¯çš„è¿‡ç¨‹ï¼Œå¹¿æ³›åº”ç”¨äºå•†ä¸šã€ç§‘å­¦å’Œç¤¾ä¼šç ”ç©¶ç­‰é¢†åŸŸã€‚æœ¬æ–‡å°†ä»‹ç»æ•°æ®æŒ–æ˜çš„åŸºæœ¬æ¦‚å¿µã€å¸¸ç”¨æŠ€æœ¯å’Œåº”ç”¨åœºæ™¯ã€‚"
published: 2025-06-19
cover: https://img.xiaozhangya.xin/file/post/arZLafej.jpg
category: "Documentation"
tags:
  - æ•°æ®æŒ–æ˜
  - DataMining
---

# å¸•ç´¯æ‰˜å›¾

- å¸•ç´¯æ‰˜å›¾ï¼ˆPareto Chartï¼‰æ˜¯ä¸€ç§ç»“åˆäº†æŸ±çŠ¶å›¾å’ŒæŠ˜çº¿å›¾çš„ç»Ÿè®¡å›¾è¡¨ï¼Œå¸¸ç”¨äºæ˜¾ç¤ºå„å› ç´ çš„é¢‘æ•°åŠå…¶ç´¯è®¡ç™¾åˆ†æ¯”ï¼Œå¸®åŠ©è¯†åˆ«ä¸»è¦é—®é¢˜æˆ–å…³é”®å› ç´ ã€‚

![å¸•ç´¯æ‰˜å›¾](https://cdn.jsdelivr.net/gh/xz131714/PicBed@main/202506141330966.png)

```python
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
plt.rcParams['font.sans-serif'] = 'SimHei'
# ç¤ºä¾‹æ•°æ®ï¼šç±»åˆ«åŠå¯¹åº”é¢‘æ•°
data = {
    'ç±»åˆ«': ['A', 'B', 'C', 'D', 'E', 'F'],
    'é¢‘æ•°': [120, 90, 60, 30, 20, 10]
}

df = pd.DataFrame(data)

# æŒ‰é¢‘æ•°é™åºæ’åº
df = df.sort_values(by='é¢‘æ•°', ascending=False).reset_index(drop=True)

# è®¡ç®—ç´¯è®¡é¢‘æ•°åŠç´¯è®¡ç™¾åˆ†æ¯”
df['ç´¯è®¡é¢‘æ•°'] = df['é¢‘æ•°'].cumsum()
df['ç´¯è®¡ç™¾åˆ†æ¯”'] = 100 * df['ç´¯è®¡é¢‘æ•°'] / df['é¢‘æ•°'].sum()

# åˆ›å»ºåŒè½´å›¾
fig, ax1 = plt.subplots()

# ç»˜åˆ¶æŸ±çŠ¶å›¾ - é¢‘æ•°
ax1.bar(df['ç±»åˆ«'], df['é¢‘æ•°'], color='C0')
ax1.set_ylabel('é¢‘æ•°', color='C0')
ax1.tick_params(axis='y', labelcolor='C0')

# åˆ›å»ºç¬¬äºŒä¸ªçºµè½´ - ç´¯è®¡ç™¾åˆ†æ¯”
ax2 = ax1.twinx()
ax2.plot(df['ç±»åˆ«'], df['ç´¯è®¡ç™¾åˆ†æ¯”'], color='C1', marker='D', ms=7)
ax2.yaxis.set_major_formatter(plt.PercentFormatter())
ax2.set_ylabel('ç´¯è®¡ç™¾åˆ†æ¯”', color='C1')
ax2.tick_params(axis='y', labelcolor='C1')
ax2.set_ylim(0, 110)

# æ ‡é¢˜å’Œå¸ƒå±€
plt.title('å¸•ç´¯æ‰˜å›¾ç¤ºä¾‹')
plt.tight_layout()
plt.show()
```

---

# ROCæ›²çº¿

- **ROCï¼ˆReceiver Operating Characteristicï¼‰æ›²çº¿** æ˜¯ç”¨äºäºŒåˆ†ç±»æ¨¡å‹æ€§èƒ½è¯„ä¼°çš„é‡è¦å·¥å…·ï¼Œç‰¹åˆ«é€‚åˆä¸å‡è¡¡æ•°æ®é›†ã€‚å®ƒåæ˜ äº†åˆ†ç±»å™¨åœ¨ä¸åŒé˜ˆå€¼ä¸‹çš„**å‡é˜³æ€§ç‡ï¼ˆFalse Positive Rate, FPRï¼‰**ä¸**çœŸé˜³æ€§ç‡ï¼ˆTrue Positive Rate, TPRï¼‰**çš„å…³ç³»ã€‚
  - **çœŸé˜³æ€§ç‡ï¼ˆTPRï¼‰**ï¼šä¹Ÿå«å¬å›ç‡ï¼Œè¡¨ç¤ºè¢«æ­£ç¡®è¯†åˆ«ä¸ºæ­£ç±»çš„æ¯”ä¾‹ã€‚è®¡ç®—å…¬å¼ï¼šTPR = TP / (TP + FN)
  - **å‡é˜³æ€§ç‡ï¼ˆFPRï¼‰**ï¼šé”™è¯¯åœ°è¢«è¯†åˆ«ä¸ºæ­£ç±»çš„è´Ÿç±»æ¯”ä¾‹ã€‚è®¡ç®—å…¬å¼ï¼šFPR = FP / (FP + TN)

- **ROCæ›²çº¿**æ˜¯ä»¥FPRä¸ºæ¨ªè½´ï¼ŒTPRä¸ºçºµè½´ç»˜åˆ¶çš„æ›²çº¿ï¼Œè¶Šæ¥è¿‘å·¦ä¸Šè§’ï¼ˆFPRä½ï¼ŒTPRé«˜ï¼‰è¯´æ˜æ¨¡å‹è¶Šå¥½ã€‚

- **AUCï¼ˆArea Under Curveï¼‰** æ˜¯ROCæ›²çº¿ä¸‹çš„é¢ç§¯ï¼Œè¡¨ç¤ºæ¨¡å‹æ•´ä½“çš„åˆ†ç±»èƒ½åŠ›ã€‚AUCè¶Šæ¥è¿‘1è¶Šå¥½ï¼Œ0.5ç›¸å½“äºéšæœºçŒœæµ‹ã€‚

![ROCæ›²çº¿](https://cdn.jsdelivr.net/gh/xz131714/PicBed@main/202506141407860.png)

```python
import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, auc
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.datasets import make_classification
import matplotlib as mpl

# è®¾ç½®ä¸­æ–‡å­—ä½“
mpl.rcParams['font.sans-serif'] = ['SimHei']  # æŒ‡å®šå­—ä½“ä¸º SimHeiï¼ˆé»‘ä½“ï¼‰
mpl.rcParams['axes.unicode_minus'] = False  # è§£å†³è´Ÿå·æ˜¾ç¤ºé—®é¢˜

# ç”Ÿæˆæ¨¡æ‹ŸäºŒåˆ†ç±»æ•°æ®é›†
X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)

# åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# è®­ç»ƒé€»è¾‘å›å½’æ¨¡å‹
model = LogisticRegression()
model.fit(X_train, y_train)

# é¢„æµ‹æµ‹è¯•é›†æ­£ç±»æ¦‚ç‡
y_score = model.predict_proba(X_test)[:, 1]

# è®¡ç®—å‡é˜³æ€§ç‡ï¼ˆFPRï¼‰ã€çœŸé˜³æ€§ç‡ï¼ˆTPRï¼‰å’Œé˜ˆå€¼
fpr, tpr, thresholds = roc_curve(y_test, y_score)

# è®¡ç®— AUC å€¼
roc_auc = auc(fpr, tpr)

# ç»˜åˆ¶ ROC æ›²çº¿
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC æ›²çº¿ (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='gray', linestyle='--', lw=2)  # éšæœºçŒœæµ‹å‚è€ƒçº¿
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('å‡é˜³æ€§ç‡ï¼ˆFalse Positive Rateï¼‰')
plt.ylabel('çœŸé˜³æ€§ç‡ï¼ˆTrue Positive Rateï¼‰')
plt.title('ROC æ›²çº¿ç¤ºä¾‹')
plt.legend(loc="lower right")
plt.show()
```



# æ•°æ®æŒ–æ˜ç®—æ³•
------
## å›å½’

### ç‰¹ç‚¹

- **ç›®æ ‡**ï¼šé¢„æµ‹è¿ç»­å€¼
- **æ¨¡å‹**ï¼šè¿ç»­å‡½æ•°  $f(X) \rightarrow Y$
- **å¸¸ç”¨æŒ‡æ ‡**ï¼šMAEã€MSEã€MedAEã€R2R^2ã€EVS

### çº¿æ€§å›å½’ (Linear Regression)

```python
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error, \
                            median_absolute_error, explained_variance_score, r2_score
import pandas as pd

# 1âƒ£ è¯»å–æ•°æ®
# è¯»å–æ•°æ®æ–‡ä»¶ 'financial.csv'ï¼Œæ•°æ®é›†åŒ…å«è‡ªå˜é‡ X å’Œå› å˜é‡ y
data = pd.read_csv("financial.csv")
X = data.drop(columns="target")   # è‡ªå˜é‡ Xï¼Œå»æ‰ç›®æ ‡åˆ— 'target'
y = data["target"]               # å› å˜é‡ yï¼Œç›®æ ‡åˆ—

# 2âƒ£ åˆ’åˆ†æ•°æ®é›†
# å°†æ•°æ®é›†æŒ‰ 80/20 çš„æ¯”ä¾‹åˆ’åˆ†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼Œéšæœºç§å­ä¸º 125
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.20, random_state=125)

# 3âƒ£ å»ºæ¨¡ä¸è®­ç»ƒ
# ä½¿ç”¨çº¿æ€§å›å½’æ¨¡å‹å¹¶å¯¹è®­ç»ƒæ•°æ®è¿›è¡Œè®­ç»ƒ
model = LinearRegression().fit(X_train, y_train)

# 4âƒ£ é¢„æµ‹
# ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹å¯¹æµ‹è¯•æ•°æ®è¿›è¡Œé¢„æµ‹
y_pred = model.predict(X_test)

# 5âƒ£ è¯„ä¼°
# è¾“å‡ºå„ç§å›å½’è¯„ä¼°æŒ‡æ ‡
print("MAE :", mean_absolute_error(y_test, y_pred))         # å¹³å‡ç»å¯¹è¯¯å·®
print("MSE :", mean_squared_error(y_test, y_pred))         # å‡æ–¹è¯¯å·®
print("MedAE :", median_absolute_error(y_test, y_pred))    # ä¸­ä½ç»å¯¹è¯¯å·®
print("EVS :", explained_variance_score(y_test, y_pred))   # è§£é‡Šæ–¹å·®å¾—åˆ†
print("RÂ² :", r2_score(y_test, y_pred))                    # RÂ² å†³å®šç³»æ•°
```

------

### é€»è¾‘å›å½’ï¼ˆå«ROCæ›²çº¿ç»˜åˆ¶ï¼‰

> è™½åä¸ºâ€œå›å½’â€ï¼Œä¹Ÿå¯ä»¥è¿›è¡Œ**äºŒå…ƒæˆ–å¤šå…ƒåˆ†ç±»**ã€‚

```python
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, \
                            confusion_matrix, roc_curve, auc
import pandas as pd
import matplotlib.pyplot as plt

# è¯»å–æ•°æ®
data = pd.read_csv("financial.csv")
X = data.drop(columns="label")  # è‡ªå˜é‡ X
y = data["label"]              # å› å˜é‡ y

# åˆ’åˆ†æ•°æ®é›†
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.20, random_state=125)

# åˆå§‹åŒ–é€»è¾‘å›å½’æ¨¡å‹å¹¶è®­ç»ƒ
clf = LogisticRegression(max_iter=1000).fit(X_train, y_train)
y_pred = clf.predict(X_test)  # å¯¹æµ‹è¯•é›†è¿›è¡Œé¢„æµ‹

# è¾“å‡ºåˆ†ç±»æŒ‡æ ‡
print("Accuracy :", accuracy_score(y_test, y_pred))        # å‡†ç¡®ç‡
print("Precision:", precision_score(y_test, y_pred))       # ç²¾ç¡®åº¦
print("Recall   :", recall_score(y_test, y_pred))          # å¬å›ç‡
print("ConfMat  :\n", confusion_matrix(y_test, y_pred))    # æ··æ·†çŸ©é˜µ

# === ROC æ›²çº¿ ===
# è®¡ç®—é¢„æµ‹ç»“æœçš„æ¦‚ç‡å¹¶ç»˜åˆ¶ ROC æ›²çº¿
y_score = clf.predict_proba(X_test)[:, 1]  # è·å–æ­£ç±»çš„æ¦‚ç‡
fpr, tpr, _ = roc_curve(y_test, y_score)   # è®¡ç®—å‡é˜³æ€§ç‡ä¸çœŸé˜³æ€§ç‡
roc_auc = auc(fpr, tpr)                    # è®¡ç®— AUC å€¼

# ç»˜åˆ¶ ROC æ›²çº¿
plt.figure()
plt.plot(fpr, tpr, lw=2, label=f"ROC (AUC={roc_auc:.2f})")
plt.plot([0, 1], [0, 1], "--")             # å‚è€ƒçº¿ï¼šå®Œå…¨éšæœº
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve â€” Logistic Regression")
plt.legend()
plt.show()
```

------

## åˆ†ç±»

### ç‰¹ç‚¹

- **è¾“å‡º**ï¼šç¦»æ•£ç±»åˆ«
- **è¾“å…¥**ï¼šæ ·æœ¬å±æ€§
- **è®­ç»ƒæ–¹å¼**ï¼šç›‘ç£å­¦ä¹ 
- **å¸¸ç”¨æŒ‡æ ‡**ï¼šAccuracyã€Precisionã€Recallã€F1ã€ROC/AUC

------

### å†³ç­–æ ‘ (Decision Tree)

```python
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix
import pandas as pd

# è¯»å–æ•°æ®
data = pd.read_excel("sales_data.xls")
X = data.drop(columns="label")  # ç‰¹å¾è‡ªå˜é‡
y = data["label"]              # ç›®æ ‡æ ‡ç­¾

# åˆ’åˆ†æ•°æ®é›†
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.20, random_state=125)

# åˆ›å»ºå†³ç­–æ ‘åˆ†ç±»å™¨å¹¶è®­ç»ƒ
dtc = DecisionTreeClassifier(criterion="entropy").fit(X_train, y_train)
y_pred = dtc.predict(X_test)  # å¯¹æµ‹è¯•é›†è¿›è¡Œé¢„æµ‹

# è¾“å‡ºåˆ†ç±»æŒ‡æ ‡
print("Accuracy :", accuracy_score(y_test, y_pred))        # å‡†ç¡®ç‡
print("Precision:", precision_score(y_test, y_pred))       # ç²¾ç¡®åº¦
print("Recall   :", recall_score(y_test, y_pred))          # å¬å›ç‡
```

------

### Kâ€¯è¿‘é‚» (Kâ€‘NN)

```python
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.datasets import load_iris

# åŠ è½½ Iris æ•°æ®é›†
iris = load_iris()
X_train, X_test, y_train, y_test = train_test_split(
    iris.data, iris.target, test_size=0.20, random_state=125)

# åˆ›å»º K-è¿‘é‚»åˆ†ç±»å™¨å¹¶è®­ç»ƒ
knn = KNeighborsClassifier(n_neighbors=5).fit(X_train, y_train)
y_pred = knn.predict(X_test)  # å¯¹æµ‹è¯•é›†è¿›è¡Œé¢„æµ‹

# è¾“å‡ºå‡†ç¡®ç‡
print("Accuracy :", accuracy_score(y_test, y_pred))  # å‡†ç¡®ç‡
```

------

### æ”¯æŒå‘é‡æœº (SVM)

```python
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.datasets import load_digits

# åŠ è½½æ‰‹å†™æ•°å­—æ•°æ®é›†
digits = load_digits()
X_train, X_test, y_train, y_test = train_test_split(
    digits.data, digits.target, test_size=0.20, random_state=125)

# åˆ›å»ºæ”¯æŒå‘é‡æœºåˆ†ç±»å™¨å¹¶è®­ç»ƒ
svc = SVC().fit(X_train, y_train)
# è¾“å‡ºå‡†ç¡®ç‡
print("Accuracy :", accuracy_score(y_test, svc.predict(X_test)))  # å‡†ç¡®ç‡
```

------

## èšç±»

### ç‰¹ç‚¹

- **æ— ç›‘ç£å­¦ä¹ **ï¼šæ— éœ€äº‹å…ˆæ ‡ç­¾
- æ ¹æ®ç›¸ä¼¼åº¦è‡ªåŠ¨åˆ’åˆ†ç°‡

### Kâ€‘Means ç¤ºä¾‹ï¼ˆå«å‚æ•°å¯»ä¼˜ï¼‰

```python
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
import pandas as pd
import matplotlib.pyplot as plt

# è¯»å–æ•°æ®å¹¶è¿›è¡Œæ ‡å‡†åŒ–
data = pd.read_excel("air_features.xlsx", index_col="ID")
data_z = (data - data.mean()) / data.std()  # Z-score æ ‡å‡†åŒ–

# ========== å›ºå®šèšç±»æ•°ä¸º5ï¼Œè¿›è¡Œèšç±»å¹¶è¾“å‡ºä¸­å¿ƒå’Œç°‡å¤§å° ==========
kmeans_5 = KMeans(n_clusters=5, random_state=3).fit(data_z)

# èšç±»ä¸­å¿ƒ + æ¯ç°‡æ•°é‡
centers = pd.DataFrame(kmeans_5.cluster_centers_, columns=data.columns)
counts  = pd.Series(kmeans_5.labels_).value_counts().sort_index().rename("æ•°é‡")
result  = pd.concat([centers, counts], axis=1)
print("K=5 èšç±»ä¸­å¿ƒåŠæ¯ç°‡æ•°é‡ï¼š")
print(result)

# ========== å¯¹ä¸åŒèšç±»æ•°åšæŒ‡æ ‡è¯„ä¼°ï¼Œç»˜åˆ¶Inertiaå’Œè½®å»“ç³»æ•° ==========
sse = []
silhouette_scores = []
K_range = range(2, 11)

for k in K_range:
    kmeans = KMeans(n_clusters=k, random_state=42).fit(data_z)
    sse.append(kmeans.inertia_)
    silhouette_scores.append(silhouette_score(data_z, kmeans.labels_))

plt.figure(figsize=(10, 4))

plt.subplot(1, 2, 1)
plt.plot(K_range, sse, 'o-', color='blue')
plt.xlabel('èšç±»æ•° K')
plt.ylabel('ç°‡å†…è¯¯å·®å¹³æ–¹å’Œ (Inertia)')
plt.title('Inertia - èšç±»æ•°ä¸ç°‡å†…è¯¯å·®å¹³æ–¹å’Œå…³ç³»')

plt.subplot(1, 2, 2)
plt.plot(K_range, silhouette_scores, 's-', color='red')
plt.xlabel('èšç±»æ•° K')
plt.ylabel('è½®å»“ç³»æ•°')
plt.title('è½®å»“ç³»æ•°æ³• - èšç±»æ•°ä¸è½®å»“ç³»æ•°å…³ç³»')
plt.tight_layout()
plt.show()
```

------

## å…³è”è§„åˆ™

### Apriori ç®—æ³•

```python
import pandas as pd
from mlxtend.preprocessing import TransactionEncoder
from mlxtend.frequent_patterns import apriori, association_rules
# è¯»å–ç”¨æˆ·è®¿é—®è®°å½•æ•°æ®
info = pd.read_csv("acc_records.csv")
# 1 å»é‡ç”¨æˆ·IP
user_ips = info["ç”¨æˆ·IP"].unique()
# 2 æ„å»ºç”¨æˆ·è®¿é—®åˆ—è¡¨
url_baskets = [
 info.loc[info["ç”¨æˆ·IP"] == ip, "URL"].tolist()
 for ip in user_ips
]
# 3 Oneâ€‘Hot ç¼–ç 
te = TransactionEncoder()
df = pd.DataFrame(te.fit_transform(url_baskets), columns=te.columns_)
# 4 é¢‘ç¹é¡¹é›† & å…³è”è§„åˆ™
freq_itemsets = apriori(df, min_support=0.005, use_colnames=True)
rules = association_rules(freq_itemsets, metric="confidence", min_threshold=0.05)
print(rules.sort_values("lift", ascending=False).head())
```

---





